Nome do estagiario: José Henrique Bernardes Vieira<br>
Data: 02/08/2024

# Continuação de Data Pipeline

Em resumo, Data Pipeline é um conjunto de processos que automatizam a transformação dos dados e o movimento entre diferentes sistemas.

## Injestão de dados
- **Batch Processing:** São os dados coletados de maneira periodica e processados em lotes. Apache Hadoop e Apache Spark são alguns softwares responsaveis para trabalhos com coleta de dados em Batch.

- **Streaming Processing:** São os dados coletados em tempo real. Apache Kafka e Apache Flink são alguns softwares responsaveis pela coleta de dados em Streaming.

## Transformação de Dados

- **ETL(Extract, transform, Load):** Os dados são extraidos de diversas fontes, transformados para um formato esperado e carregados para o destino final. Algumas ferramentas utilizadas para esse tipo de processo é o Apache Nifi, Talend e Informatica.

- **ELT(Extract, load, transform):** Os dados são extraidos, depois carregados em um data warehouse e então depois transformados. As ferramentas utilizadas para esse processo é o Snowflacke e Google BigQuery.

## Armazenamento de dados

- **Data Lakes:** Em resumo data lakes são responsaveis por armazenar os dados brutos em seu formato original. Os serviços utilizados para esse tipo de trabalho é o Amazon S3 e o Azure Data Lake.

- **Data Warehouse:** Em resumo Data Warehouse são responsaveis por armazenar dados estruturados e processados para análise. Os serviços utilizados para esse tipo de trabalho é o Amazon Redshift e o Google BigQuery.

## Orquestração e Monitoramento

- **Orquestração:** É o gerenciamento e o agendamento de tarefas do data pipeline. As ferramentas utilizadas é o Apache Airflow, Prefect e Dagster.

- **Monitoramento:** É o rastreamento de desemoenho, as falhas e a integridade do data pipeline. As ferramentas utilizadas é o Prometheus e o Grafana.

## Segurança e governança de dados

- **Segurança:** É a garantir a proteção dos dados em repouso e em repouso. Os exemplos é a criptografia e o controle de acesso.

- **Governança:** É manter a qualidade e a conformidade dos dados.
As ferramentas utilizadas é o Apache Atlas e o Colibra.

## Exemplos de ferramentas e tecnologias

- **Apache Kafka:** É uma plataforma de streaming distribuido que permite a publicação e subscrição de fluxos e registro, armazenamento de registros de maneira tolerante a falhas e processamento de fluxos de registro em tempo real.

- **Apache Spark:** Um sistema de computação em cluster rápido para processamento de dados em grande escala.

- **Airflow:** Uma plataforma para criar, agendar e monitorar workflows.
DBT (Data Build Tool): Uma ferramenta que permite transformar dados dentro de um data warehouse.

- **Amazon Redshift:** Um data warehouse totalmente gerenciado na nuvem que permite a execução de consultas complexas de forma rápida e eficiente.

## Benefícios de um Data Pipeline

- **Automação:** Reduz a necessidade de intervenções manuais e minimiza erros.

- **Escalabilidade:** Facilita o manuseio de grandes volumes de dados.

- **Agilidade:** Acelera o tempo de preparação e análise de dados.
Confiabilidade: Garante a consistência e a integridade dos dados através de processos padronizados.