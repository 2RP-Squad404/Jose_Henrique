Nome do estagiario: José Henrique Bernardes Vieira<br>
Data: 02/08/2024

# Continuação de Data Pipeline

Em resumo, Data Pipeline é um conjunto de processos que automatizam a transformação dos dados e o movimento entre diferentes sistemas.

## Injestão de dados
- **Batch Processing:** São os dados coletados de maneira periodica e processados em lotes. Apache Hadoop e Apache Spark são alguns softwares responsaveis para trabalhos com coleta de dados em Batch.

- **Streaming Processing:** São os dados coletados em tempo real. Apache Kafka e Apache Flink são alguns softwares responsaveis pela coleta de dados em Streaming.

## Transformação de Dados

- **ETL(Extract, transform, Load):** Os dados são extraidos de diversas fontes, transformados para um formato esperado e carregados para o destino final. Algumas ferramentas utilizadas para esse tipo de processo é o Apache Nifi, Talend e Informatica.

- **ELT(Extract, load, transform):** Os dados são extraidos, depois carregados em um data warehouse e então depois transformados. As ferramentas utilizadas para esse processo é o Snowflacke e Google BigQuery.

## Armazenamento de dados

- **Data Lakes:** Em resumo data lakes são responsaveis por armazenar os dados brutos em seu formato original. Os serviços utilizados para esse tipo de trabalho é o Amazon S3 e o Azure Data Lake.

- **Data Warehouse:** Em resumo Data Warehouse são responsaveis por armazenar dados estruturados e processados para análise. Os serviços utilizados para esse tipo de trabalho é o Amazon Redshift e o Google BigQuery.

## Orquestração e Monitoramento

- **Orquestração:** É o gerenciamento e o agendamento de tarefas do data pipeline. As ferramentas utilizadas é o Apache Airflow, Prefect e Dagster.

- **Monitoramento:** É o rastreamento de desempenho, as falhas e a integridade do data pipeline. As ferramentas utilizadas é o Prometheus e o Grafana.

## Segurança e governança de dados

- **Segurança:** É a garantir a proteção dos dados em repouso e em trânsito. Os exemplos é a criptografia e o controle de acesso.

- **Governança:** É manter a qualidade e a conformidade dos dados.
As ferramentas utilizadas é o Apache Atlas e o Colibra.

## Exemplos de ferramentas e tecnologias

- **Apache Kafka:** É uma plataforma de streaming distribuido que permite a publicação e subscrição de fluxos e registro, armazenamento de registros de maneira tolerante a falhas e processamento de fluxos de registro em tempo real.

- **Apache Spark:** Um sistema de computação em cluster rápido para processamento de dados em grande escala.

- **Airflow:** Uma plataforma para criar, agendar e monitorar workflows.
DBT (Data Build Tool): Uma ferramenta que permite transformar dados dentro de um data warehouse.

- **Amazon Redshift:** Um data warehouse totalmente gerenciado na nuvem que permite a execução de consultas complexas de forma rápida e eficiente.

## Benefícios de um Data Pipeline

- **Automação:** Reduz a necessidade de intervenções manuais e minimiza erros.

- **Escalabilidade:** Facilita o manuseio de grandes volumes de dados.

- **Agilidade:** Acelera o tempo de preparação e análise de dados.
Confiabilidade: Garante a consistência e a integridade dos dados através de processos padronizados.

# Continuação de Data Lake.

Data Lake é uma solução de armazenamento de dados centralizado que permite guardar dados brutos em seu formato original, dados estruturados, dados não estruturados e dados semi-estruturados. Esse tipo é muito utilizado em cenários de big data e análise avançada.

## Componentes de um Data Lake:

### Armazenamento de dados

- **Objetivo:** Armazenar dados em seu estado bruto, sem ter a necessidade de limpar ou transformar os dados previamente.

- **Tecnologias:** Um Data Lake pode usar serviços da Amazon S3 (Simple Storage Service) que é um serviço de armazenamento de objetos altamente escalável da AWS, ou o Azure Data Lake Storage um serviço de armazenamento de dados otimizado para análise na Azure ou também Google Cloud Storage um serviço de armazenamento de objetos do Google Cloud, esses serviços são muito utilizados para armazenamento de dados em Data Lakes.

### Injestão de dados

- **Objetivo:** Capturar e armazenar dados de varias fontes de entrada.

- **Tecnologias:** As tecnologias que podem ser usadas para a Injestão de dados em um Data Lake é o Apache Kafka que é uma plataforma de streaming distribuído para captura de dados em tempo real, o Apache NiFi uma ferramenta para automatizar o fluxo de dados entre sistemas e também o AWS Glue um serviço de ETL (Extract, Transform, Load) gerenciado da AWS.

### Catálogo e Metadados

- **Objetivo:**  Gerenciar e catalogar dados armazenados, facilitando a descoberta e governança de dados.

- **Tecnologias:** As tecnologias utilizadas para esse tipo de trabalho é o
AWS Glue Data Catalog um Catálogo de dados para armazenar metadados e definir esquemas de dados, o Apache Atlas é uma ferramenta de governança de dados e metadados e também o Azure Data Catalog um Serviço de catálogo de metadados na Azure.

## Processamento de Dados

- **Objetivo:** Processar, transformar e analisar dados armazenados.

- **Tecnologias:** Para processamento de dados as tecnologias utilizadas é o
Apache Spark um Framework de processamento de dados em grande escala, o Databricks que é uma plataforma de análise de dados baseada no Apache Spark e também AWS Lambda é um serviço de computação sem servidor para executar código em resposta a eventos.

## Segurança e Governança

- **Objetivo:** Proteger dados e garantir conformidade com regulamentações.

- **Tecnologias:** As tecnologias usadas para proteção e conformidade dos dados é o AWS IAM (Identity and Access Management) um controle de acesso e gerenciamento de identidade na AWS, Azure Active Directory é um serviço de gerenciamento de identidades e acessos na Azure e também o Google Cloud IAM um controle de acesso e gerenciamento de identidade no Google Cloud.

## Análise e Visualização

- **Objetivo:** Extrair insights e visualizar dados para tomada de decisão.

- **Tecnologias:** As tecnologias usadas para esse trabalho é o Amazon Athena um serviço de consulta interativa que permite analisar dados diretamente no Amazon S3, o Azure Synapse Analytics que é um serviço de análise que integra big data e data warehousing e também o Google BigQuery um data warehouse gerenciado que permite análises rápidas de grandes volumes de dados.

## Exemplos de uso

- **Analise de Logs:** Um Data Lake pode ser usado para armazenar e processar grandes volumes de logs em servidores para detectar padrões e anomalias

- **IoT:** Pode ser utilizados em capturar dados de sensores em tempo real para analise preditiva e armzená-los.

- **Midia Social:** Sua utilização na midia social funciona para analisar dados para entender as tendências e comportamento dos usuarios.

## Beneficios de um Data Lake

- **Escalabilidade:** O Data Lake possui a capacidade de armazenar grandes volumes de dados de forma econômica.

- **Flexibilidade:** Possui suporte para multiplos formatos sendo eles, estruturados, não estruturados e semi-estruturados.

- **Análise avançada:** Tem possibilidade de aplicar tecnicas de machine learning e analise de big data diretamente sobre os dados brutos.

## Desafios
- **Gerenciamento de Metadados:** Manter um catálogo de metadados preciso e atualizado.

- **Segurança e Conformidade:** Garantir a proteção dos dados sensíveis e conformidade com regulamentações.

- **Qualidade dos Dados:** Implementar práticas de governança para manter a qualidade dos dados armazenados.
