Nome do estagiario: José Henrique Bernardes Vieira<br>
Data: 02/08/2024

# Continuação de Data Pipeline

Em resumo, Data Pipeline é um conjunto de processos que automatizam a transformação dos dados e o movimento entre diferentes sistemas.

## Injestão de dados
- **Batch Processing:** São os dados coletados de maneira periodica e processados em lotes. Apache Hadoop e Apache Spark são alguns softwares responsaveis para trabalhos com coleta de dados em Batch.

- **Streaming Processing:** São os dados coletados em tempo real. Apache Kafka e Apache Flink são alguns softwares responsaveis pela coleta de dados em Streaming.

## Transformação de Dados

- **ETL(Extract, transform, Load):** Os dados são extraidos de diversas fontes, transformados para um formato esperado e carregados para o destino final. Algumas ferramentas utilizadas para esse tipo de processo é o Apache Nifi, Talend e Informatica.

- **ELT(Extract, load, transform):** Os dados são extraidos, depois carregados em um data warehouse e então depois transformados. As ferramentas utilizadas para esse processo é o Snowflacke e Google BigQuery.

## Armazenamento de dados

- **Data Lakes:** Em resumo data lakes são responsaveis por armazenar os dados brutos em seu formato original. Os serviços utilizados para esse tipo de trabalho é o Amazon S3 e o Azure Data Lake.

- **Data Warehouse:** Em resumo Data Warehouse são responsaveis por armazenar dados estruturados e processados para análise. Os serviços utilizados para esse tipo de trabalho é o Amazon Redshift e o Google BigQuery.

## Orquestração e Monitoramento

- **Orquestração:** É o gerenciamento e o agendamento de tarefas do data pipeline. As ferramentas utilizadas é o Apache Airflow, Prefect e Dagster.

- **Monitoramento:** É o rastreamento de desempenho, as falhas e a integridade do data pipeline. As ferramentas utilizadas é o Prometheus e o Grafana.

## Segurança e governança de dados

- **Segurança:** É a garantir a proteção dos dados em repouso e em trânsito. Os exemplos é a criptografia e o controle de acesso.

- **Governança:** É manter a qualidade e a conformidade dos dados.
As ferramentas utilizadas é o Apache Atlas e o Colibra.

## Exemplos de ferramentas e tecnologias

- **Apache Kafka:** É uma plataforma de streaming distribuido que permite a publicação e subscrição de fluxos e registro, armazenamento de registros de maneira tolerante a falhas e processamento de fluxos de registro em tempo real.

- **Apache Spark:** Um sistema de computação em cluster rápido para processamento de dados em grande escala.

- **Airflow:** Uma plataforma para criar, agendar e monitorar workflows.
DBT (Data Build Tool): Uma ferramenta que permite transformar dados dentro de um data warehouse.

- **Amazon Redshift:** Um data warehouse totalmente gerenciado na nuvem que permite a execução de consultas complexas de forma rápida e eficiente.

## Benefícios de um Data Pipeline

- **Automação:** Reduz a necessidade de intervenções manuais e minimiza erros.

- **Escalabilidade:** Facilita o manuseio de grandes volumes de dados.

- **Agilidade:** Acelera o tempo de preparação e análise de dados.
- **Confiabilidade:** Garante a consistência e a integridade dos dados através de processos padronizados.

# Continuação de Data Lake.

Data Lake é uma solução de armazenamento de dados centralizado que permite guardar dados brutos em seu formato original, dados estruturados, dados não estruturados e dados semi-estruturados. Esse tipo é muito utilizado em cenários de big data e análise avançada.

## Componentes de um Data Lake:

### Armazenamento de dados

- **Objetivo:** Armazenar dados em seu estado bruto, sem ter a necessidade de limpar ou transformar os dados previamente.

- **Tecnologias:** Um Data Lake pode usar serviços da Amazon S3 (Simple Storage Service) que é um serviço de armazenamento de objetos altamente escalável da AWS, ou o Azure Data Lake Storage um serviço de armazenamento de dados otimizado para análise na Azure ou também Google Cloud Storage um serviço de armazenamento de objetos do Google Cloud, esses serviços são muito utilizados para armazenamento de dados em Data Lakes.

### Injestão de dados

- **Objetivo:** Capturar e armazenar dados de varias fontes de entrada.

- **Tecnologias:** As tecnologias que podem ser usadas para a Injestão de dados em um Data Lake é o Apache Kafka que é uma plataforma de streaming distribuído para captura de dados em tempo real, o Apache NiFi uma ferramenta para automatizar o fluxo de dados entre sistemas e também o AWS Glue um serviço de ETL (Extract, Transform, Load) gerenciado da AWS.

### Catálogo e Metadados

- **Objetivo:**  Gerenciar e catalogar dados armazenados, facilitando a descoberta e governança de dados.

- **Tecnologias:** As tecnologias utilizadas para esse tipo de trabalho é o
AWS Glue Data Catalog um Catálogo de dados para armazenar metadados e definir esquemas de dados, o Apache Atlas é uma ferramenta de governança de dados e metadados e também o Azure Data Catalog um Serviço de catálogo de metadados na Azure.

## Processamento de Dados

- **Objetivo:** Processar, transformar e analisar dados armazenados.

- **Tecnologias:** Para processamento de dados as tecnologias utilizadas é o
Apache Spark um Framework de processamento de dados em grande escala, o Databricks que é uma plataforma de análise de dados baseada no Apache Spark e também AWS Lambda é um serviço de computação sem servidor para executar código em resposta a eventos.

## Segurança e Governança

- **Objetivo:** Proteger dados e garantir conformidade com regulamentações.

- **Tecnologias:** As tecnologias usadas para proteção e conformidade dos dados é o AWS IAM (Identity and Access Management) um controle de acesso e gerenciamento de identidade na AWS, Azure Active Directory é um serviço de gerenciamento de identidades e acessos na Azure e também o Google Cloud IAM um controle de acesso e gerenciamento de identidade no Google Cloud.

## Análise e Visualização

- **Objetivo:** Extrair insights e visualizar dados para tomada de decisão.

- **Tecnologias:** As tecnologias usadas para esse trabalho é o Amazon Athena um serviço de consulta interativa que permite analisar dados diretamente no Amazon S3, o Azure Synapse Analytics que é um serviço de análise que integra big data e data warehousing e também o Google BigQuery um data warehouse gerenciado que permite análises rápidas de grandes volumes de dados.

## Exemplos de uso

- **Analise de Logs:** Um Data Lake pode ser usado para armazenar e processar grandes volumes de logs em servidores para detectar padrões e anomalias

- **IoT:** Pode ser utilizados em capturar dados de sensores em tempo real para analise preditiva e armzená-los.

- **Midia Social:** Sua utilização na midia social funciona para analisar dados para entender as tendências e comportamento dos usuarios.

## Beneficios de um Data Lake

- **Escalabilidade:** O Data Lake possui a capacidade de armazenar grandes volumes de dados de forma econômica.

- **Flexibilidade:** Possui suporte para multiplos formatos sendo eles, estruturados, não estruturados e semi-estruturados.

- **Análise avançada:** Tem possibilidade de aplicar tecnicas de machine learning e analise de big data diretamente sobre os dados brutos.

## Desafios
- **Gerenciamento de Metadados:** Manter um catálogo de metadados preciso e atualizado.

- **Segurança e Conformidade:** Garantir a proteção dos dados sensíveis e conformidade com regulamentações.

- **Qualidade dos Dados:** Implementar práticas de governança para manter a qualidade dos dados armazenados.

# Continuação Data Warehouse

Em um Data Warehouse armazena dados transformados, organizados e otimizados para uma consulta e analise rápida.

## Componentes de um Data Warehouse

### ETL(Extract, Transform, Load)

- **Objetivo:** Extrair dados de diversas fontes, transformá-los em um formato adequado e carregá-los no data warehouse.

- **Tecnologias:** As tecnologias utilizadas para executar um ETL pode ser o Apache NiFi uma ferramenta de integração de dados para automação de fluxo de dados, Informatica PowerCenter que é uma plataforma de integração de dados e também o AWS Glue um serviço de ETL gerenciado na nuvem.

### Data Warehouse

- **Objetivo:** Armazenar dados transformados em um formato estruturado, otimizando-os para consultas e relatórios.

- **Tecnologias:** As tecnologias usadas para data warehouse é o Amazon Redshift um data warehouse gerenciado da AWS, o Google BigQuery que é um serviço de data warehouse gerenciado e altamente escalável do Google Cloud, Snowflake uma Plataforma de data warehouse em nuvem que permite armazenamento e análise de dados e também o Microsoft Azure Synapse Analytics uma Plataforma de análise que combina data warehousing com big data.

### Ferramentas de Análise e BI (Business Intelligence)

- **Objetivo:** Permitir a visualização, análise e geração de relatórios dos dados armazenados no data warehouse.

- **Tecnologias:**As tecnologias usadas em analises e BI são Tableau uma ferramenta de visualização de dados que se integra com data warehouses para criar dashboards interativos, Microsoft Power BI que é uma ferramenta de BI que permite a criação de relatórios e dashboards e o Looker uma Ferramenta de análise de dados que se conecta diretamente ao data warehouse.

## Gestão de Metadados

- **Objetivo:** Manter informações sobre os dados armazenados, incluindo origem, transformações aplicadas e estrutura.

- **Tecnologias:** As tecnologias para esse trabalho é o Apache Atlas uma ferramenta para gerenciamento de metadados e governança de dados e o Collibra uma plataforma de governança de dados que inclui recursos de gestão de metadados.

## Segurança e Governança

- **Objetivo:** Proteger os dados armazenados e garantir que as práticas de conformidade sejam seguidas.

- **Tecnologias:** As tecnolgia para esses trabalhos é o AWS IAM (Identity and Access Management) para controle de acesso e gerenciamento de identidades para recursos na AWS, Azure Active Directory um Serviço de gerenciamento de identidades e acessos na Azure e também o Data Masking uma técnica para anonimizar dados sensíveis.

## Exemplos de uso de um data warehouse

- **Analise de vendas:** As empresas de varejo utilizam para analize de padrões de venda e comportamento dos clientes.

- **Relatórios financeiros:** As intistuições financeiras utilizam para consolidar e analisar dados financeiros de diversas fontes.

- **Marketing e CMR:** Empresas de Marketing utilizam para analise de campanhas publicitarias e engajamento dos clientes.

## Beneficios de um data warehouse

- **Alta performance:** O Data Warehouse é otimizado para consultas rápidas e complexas sobre grande volumes de dados

- **Consistencia dos dados:** O uso de um Data Warehouse oferece uma visão unica e consistente dos dados, inegrando informações de diferentes fontes.

- **Suporte a tomada de decisão:** Facilita a analisede dados históricos, ajudando as empresas a tomar decisões mais informadas.

## Desafios

- **Custo:** Um data warehouse pode ter um custo de configuração e manutenção altos, principlmente quando são em larga escala.

- **Complexidade:** Projetar e implementar um data warehouse requer planejamento cuidadoso e expertise técnica.
